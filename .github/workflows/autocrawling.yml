# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python
name: Crawling

on:
  schedule:
    - cron: '0 */3 * * *' # 3시간 마다 실행
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'warning'
        type: choice
        options:
          - info
          - warning
          - debug
permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python 3.7
        uses: actions/setup-python@v2
        with:
          python-version: "3.7"

      - name: Install dependencies
        working-directory: CarrotMarket/JiHyeon/AutoCrawling/
        run: |
          python -m pip install --upgrade pip
          pip install requests
          pip install beautifulsoup4
          pip install newspaper3k
          pip install pandas
          pip install schedule

      - name: Run crawling.py
        working-directory: CarrotMarket/JiHyeon/AutoCrawling/
        run: |
          python BusNewCrawling.py
          python CctvNewsCrawling.py
          python SubwayNewsCrawling.py
          python SecuritylightNewsCrawling.py
          python PoliceNewsCrawling.py
          python EmergenbellNewsCrawling.py

      - name: Commits and Push
        run: |
          git config --local user.email "gus1043@gmail.com"
          git config --local user.name "Auto_Scraping_Name"
          git add .
          git diff --cached --quiet || git commit -m "Auto - Update data with Scraping"
          git pull origin main # 최신 변경 사항 가져오기
          git push origin main # 변경된 내용 푸시
        env:
          GITHUB_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}


      # - name: Upload CSV to Hadoop
      #   run: |
      #     # Install Google Cloud SDK
      #     curl https://sdk.cloud.google.com | bash
      #     source /home/runner/google-cloud-sdk/path.bash.inc
      
      #     # Decode the private key
      #     echo "${{ secrets.HADOOP_SSH_PRIVATE_KEY }}" | base64 --decode > gcp_key.json
      
      #     # Activate service account and set project
      #     gcloud auth activate-service-account --key-file=gcp_key.json
      #     gcloud config set project winged-sol-407114
      
      #     # Use gcloud compute scp command to copy CSV files to Hadoop directory on GCP VM instance
      #     gcloud compute scp --recurse CarrotMarket/JiHyeon/AutoCrawling/*.csv maria_dev@instance-1:/home/maria_dev/crawling/
      #   env:
      #     HADOOP_USER_NAME: maria_dev
      #     HADOOP_SSH_PRIVATE_KEY: ${{ secrets.HADOOP_SSH_PRIVATE_KEY }}
