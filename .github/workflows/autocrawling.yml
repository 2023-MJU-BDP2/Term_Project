# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python
name: Crawling

on:
  schedule:
    - cron: '*/5 * * * *' # Every 5 minutes
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'warning'
        type: choice
        options:
          - info
          - warning
          - debug

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python 3.7
        uses: actions/setup-python@v2
        with:
          python-version: "3.7"

      - name: Install dependencies
        working-directory: CarrotMarket/JiHyeon/AutoCrawling/
        run: |
          python -m pip install --upgrade pip
          pip install requests
          pip install beautifulsoup4
          pip install newspaper3k
          pip install pandas
          pip install schedule

      - name: Run crawling.py
        working-directory: CarrotMarket/JiHyeon/AutoCrawling/
        run: |
          python BusNewCrawling.py
          python CctvNewsCrawling.py
          python SubwayNewsCrawling.py
          python SecuritylightNewsCrawling.py
          python PoliceNewsCrawling.py
          python EmergenbellNewsCrawling.py

      - name: Upload CSV to Hadoop
        run: |
          # Use gcloud compute scp command to copy CSV files to Hadoop directory on GCP VM instance
          gcloud compute scp --recurse CarrotMarket/JiHyeon/AutoCrawling/*.csv maria_dev@instance-1:/home/maria_dev/crawling/
        env:
          HADOOP_USER_NAME: maria_dev
          HADOOP_SSH_PRIVATE_KEY: ${{ secrets.HADOOP_SSH_PRIVATE_KEY }}
          HADOOP_SSH_PRIVATE_KEY: ${{ secrets.HADOOP_SSH_PRIVATE_KEY }}
